{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "train_generation_progressive.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHhhplL7opm"
      },
      "source": [
        "# Train a neural network for mri generation using progressive growing\n",
        "\n",
        "In this notebook, we will use Nobrainer to train a model for brain MRI generation. Brain MRI generation is a useful task in synthetically creating neuroimaging data. We will use a Generative Adversarial Network to model the generation and use a progressive growing training method for high quality generation at higher resolutions.\n",
        "\n",
        "In the following cells, we will:\n",
        "\n",
        "1. Get sample T1-weighted MR scans as features.\n",
        "2. Convert the data to TFRecords format.\n",
        "3. Instantiate a progressive convolutional neural network for generator and discriminator.\n",
        "4. Create a Dataset of the features.\n",
        "5. Instantiate a trainer and choose a loss function to use.\n",
        "6. Train on part of the data in two phases (transition and resolution).\n",
        "7. Repeat steps 4-6 for each growing resolution.\n",
        "8. Generate some images using trained model\n",
        "\n",
        "## Google Colaboratory\n",
        "\n",
        "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU greatly speeds up training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUwl5vYH7rrD",
        "outputId": "f4cab389-dccf-4a2e-c62f-b7887d9978d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --no-cache-dir https://github.com/neuronets/nobrainer/archive/refs/heads/add-progressivegan.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/neuronets/nobrainer/archive/refs/heads/add-progressivegan.zip\n",
            "\u001b[?25l  Downloading https://github.com/neuronets/nobrainer/archive/refs/heads/add-progressivegan.zip\n",
            "\r\u001b[K     - 10kB 24.9MB/s\r\u001b[K     \\ 20kB 37.2MB/s\r\u001b[K     | 30kB 35.8MB/s\r\u001b[K     / 40kB 34.7MB/s\r\u001b[K     - 51kB 30.0MB/s\r\u001b[K     \\ 61kB 19.0MB/s\r\u001b[K     | 71kB 11.0MB/s\r\u001b[K     / 81kB 11.9MB/s\r\u001b[K     - 92kB 12.2MB/s\r\u001b[K     \\ 102kB 12.7MB/s\r\u001b[K     | 112kB 12.7MB/s\r\u001b[K     / 122kB 12.7MB/s\r\u001b[K     - 133kB 12.7MB/s\r\u001b[K     \\ 143kB 12.7MB/s\r\u001b[K     | 153kB 12.7MB/s\r\u001b[K     / 163kB 12.7MB/s\r\u001b[K     - 174kB 12.7MB/s\r\u001b[K     \\ 184kB 12.7MB/s\r\u001b[K     | 194kB 12.7MB/s\r\u001b[K     / 204kB 12.7MB/s\r\u001b[K     - 215kB 12.7MB/s\r\u001b[K     \\ 225kB 12.7MB/s\r\u001b[K     | 235kB 12.7MB/s\r\u001b[K     / 245kB 12.7MB/s\r\u001b[K     - 256kB 12.7MB/s\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): nobrainer===refs-pull-138-head from https://github.com/neuronets/nobrainer/archive/refs/heads/add-progressivegan.zip in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: tensorflow-probability>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from nobrainer===refs-pull-138-head) (0.12.1)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from nobrainer===refs-pull-138-head) (2.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nobrainer===refs-pull-138-head) (5.4.8)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from nobrainer===refs-pull-138-head) (3.0.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from nobrainer===refs-pull-138-head) (0.16.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nobrainer===refs-pull-138-head) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nobrainer===refs-pull-138-head) (1.19.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.11.0->nobrainer===refs-pull-138-head) (4.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.11.0->nobrainer===refs-pull-138-head) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.11.0->nobrainer===refs-pull-138-head) (1.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.11.0->nobrainer===refs-pull-138-head) (0.1.6)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.11.0->nobrainer===refs-pull-138-head) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (0.12.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.34.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.12.4)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nobrainer===refs-pull-138-head) (2.5.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nobrainer===refs-pull-138-head) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nobrainer===refs-pull-138-head) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nobrainer===refs-pull-138-head) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nobrainer===refs-pull-138-head) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nobrainer===refs-pull-138-head) (1.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (57.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.30.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (0.4.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer===refs-pull-138-head) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer===refs-pull-138-head) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer===refs-pull-138-head) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer===refs-pull-138-head) (1.3.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (4.7.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.4.0->nobrainer===refs-pull-138-head) (3.1.0)\n",
            "Building wheels for collected packages: nobrainer\n",
            "  Building wheel for nobrainer (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nobrainer: filename=nobrainer-refs_pull_138_head-cp37-none-any.whl size=75533 sha256=7ef029c7c2cd3c0786aedaa4261162f5575692ab4982a85b24d457411467ccda\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jrnjt4_x/wheels/2d/54/56/dce9de493f82c31568a1abdec83e7e0b36db5c4e4eb8e4b9e1\n",
            "Successfully built nobrainer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpm8u9O47opq"
      },
      "source": [
        "import nobrainer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAl3sk8e7opr"
      },
      "source": [
        "# Get sample features and labels\n",
        "\n",
        "We use 9 pairs of volumes for training and 1 pair of volumes for evaluation. Many more volumes would be required to train a model for any useful purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV9F64HE7opr"
      },
      "source": [
        "csv_of_filepaths = nobrainer.utils.get_data()\n",
        "filepaths = nobrainer.io.read_csv(csv_of_filepaths)\n",
        "\n",
        "train_paths = filepaths[:9]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N58H-gun7opt"
      },
      "source": [
        "# Set the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6yR0Cq37opu"
      },
      "source": [
        "latent_size = 1024\n",
        "g_fmap_base = 4096\n",
        "d_fmap_base = 4096\n",
        "num_parallel_calls = 4\n",
        "iterations = int(300e3)\n",
        "lr = 1e-4\n",
        "\n",
        "resolution_batch_size_map = {8: 32, 16: 16, 32: 8, 64: 4, 128: 1, 256: 1} \n",
        "resolutions = sorted(list(resolution_batch_size_map.keys()))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqzlAmyI7ops"
      },
      "source": [
        "# Convert medical images to TFRecords\n",
        "\n",
        "Remember how many full volumes are in the TFRecords files. This will be necessary to know how many steps are in on training epoch. The default training method needs to know this number, because Datasets don't always know how many items they contain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PvvTXmO7ops"
      },
      "source": [
        "!mkdir -p data"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFJ1Yl9y7opt",
        "outputId": "b791e106-8f01-4c7a-c94a-28f4e1b48a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        }
      },
      "source": [
        "nobrainer.tfrecord.write(\n",
        "    features_labels=train_paths,\n",
        "    filename_template='data/data-train_shard-{shard:03d}.tfrec',\n",
        "    examples_per_shard=3,\n",
        "    multi_resolution=True,\n",
        "    resolutions=resolutions)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0/3 [..............................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\", line 99, in __writer_func\n    map_fn(protobuf_iterator=iterator, filename=filename)\n  File \"/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\", line 399, in _write_tfrecords\n    for proto_string_dict in protobuf_iterator:\n  File \"/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\", line 318, in __next__\n    serialized = self._serialize(index)\n  File \"/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\", line 363, in _serialize\n    feature=x, label=y, feature_affine=affine, label_affine=None\n  File \"/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\", line 263, in _to_proto\n    d.update(_get_label_dict(y=label, affine=label_affine))\n  File \"/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\", line 226, in _get_label_dict\n    y = y.astype(_TFRECORDS_DTYPE)\nValueError: could not convert string to float: '/tmp/nobrainer-data/datasets/sub-01_aparc+aseg.mgz'\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b26049067f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mexamples_per_shard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmulti_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     resolutions=resolutions)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(features_labels, filename_template, examples_per_shard, to_ras, compressed, processes, chunksize, multi_resolution, resolutions, verbose)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fork\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         for _ in p.imap_unordered(\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0m__writer_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         ):\n\u001b[1;32m    109\u001b[0m             \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\u001b[0m in \u001b[0;36m__writer_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__writer_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotobuf_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\u001b[0m in \u001b[0;36m_write_tfrecords\u001b[0;34m()\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mproto_string_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprotobuf_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mresolution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolutions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mtf_record_writers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_string_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mserialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_j\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\u001b[0m in \u001b[0;36m_serialize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    361\u001b[0m                 )\n\u001b[1;32m    362\u001b[0m                 proto = _to_proto(\n\u001b[0;32m--> 363\u001b[0;31m                     \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_affine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_affine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m                 )\n\u001b[1;32m    365\u001b[0m                 \u001b[0mproto_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\u001b[0m in \u001b[0;36m_to_proto\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;34m\"\"\"Return instance of `tf.train.Example` of feature and label.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_feature_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_affine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_label_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_affine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nobrainer/tfrecord.py\u001b[0m in \u001b[0;36m_get_label_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_label_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_TFRECORDS_DTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     label = {\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '/tmp/nobrainer-data/datasets/sub-01_aparc+aseg.mgz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CUSYF427opu"
      },
      "source": [
        "# Create logging directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghqyOT587opv"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "save_dir = 'temp'\n",
        "\n",
        "save_dir = Path(save_dir)\n",
        "generated_dir = save_dir.joinpath('generated')\n",
        "model_dir = save_dir.joinpath('saved_models')\n",
        "log_dir = save_dir.joinpath('logs')\n",
        "\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "generated_dir.mkdir(exist_ok=True)\n",
        "model_dir.mkdir(exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJu8zPdr7opv"
      },
      "source": [
        "# Instantiate a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2uWJUa7opv"
      },
      "source": [
        "generator, discriminator = nobrainer.models.progressivegan(latent_size, \n",
        "                                                           g_fmap_base=g_fmap_base, \n",
        "                                                           d_fmap_base=d_fmap_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl7FeQGm7opw"
      },
      "source": [
        "# Train the network progressively for each resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvdWJq6p7opw"
      },
      "source": [
        "for resolution in resolutions:\n",
        "    \n",
        "    # create a train dataset with features for resolution\n",
        "    dataset_train = nobrainer.dataset.get_dataset(\n",
        "        file_pattern=\"data/*res-%03d*.tfrec\"%(resolution),\n",
        "        batch_size=resolution_batch_size_map[resolution],\n",
        "        num_parallel_calls=num_parallel_calls,\n",
        "        volume_shape=(resolution, resolution, resolution),\n",
        "        n_classes=1, # dummy labels as this is unsupervised training\n",
        "        scalar_label=True,\n",
        "        standardize=False\n",
        "    )\n",
        "\n",
        "    # grow the networks by one (2^x) resolution\n",
        "    generator.add_resolution()\n",
        "    discriminator.add_resolution()\n",
        "\n",
        "    # instantiate a progressive training helper\n",
        "    progressive_gan_trainer = nobrainer.training.ProgressiveGANTrainer(\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "        gradient_penalty=True)\n",
        "\n",
        "    # compile with optimizers and loss function of choice\n",
        "    progressive_gan_trainer.compile(\n",
        "        g_optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.0, beta_2=0.99, epsilon=1e-8),\n",
        "        d_optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.0, beta_2=0.99, epsilon=1e-8),\n",
        "        g_loss_fn=nobrainer.losses.wasserstein,\n",
        "        d_loss_fn=nobrainer.losses.wasserstein\n",
        "        )\n",
        "\n",
        "    steps_per_epoch = iterations//resolution_batch_size_map[resolution]\n",
        "    # save_best_only is set to False as it is an adversarial loss\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(str(model_dir), save_weights_only=True, save_best_only=False, save_freq=10)\n",
        "\n",
        "    # Train at resolution\n",
        "    print('Resolution : {}'.format(resolution))\n",
        "\n",
        "    print('Transition phase')\n",
        "    progressive_gan_trainer.fit(\n",
        "        dataset_train,\n",
        "        phase='transition',\n",
        "        resolution=resolution,\n",
        "        steps_per_epoch=steps_per_epoch, # necessary for repeat dataset\n",
        "        callbacks=[model_checkpoint_callback])\n",
        "\n",
        "    print('Resolution phase')\n",
        "    progressive_gan_trainer.fit(\n",
        "        dataset_train,\n",
        "        phase='resolution',\n",
        "        resolution=resolution,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=[model_checkpoint_callback])\n",
        "\n",
        "    # save the final weights\n",
        "    generator.save(str(model_dir.joinpath('generator_final_res_{}'.format(resolution))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}